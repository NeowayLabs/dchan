#+CAPTION: Travis-CI status
#+NAME: travis-build-status
[[https://travis-ci.org/NeowayLabs/dchan.svg?branch=master]]

* dchan

  Dchan is a server that exposes channels for inter-process
  communications (IPC) over a file tree interface.  These channels are
  much like Go channels and can be used in the same way but between
  processes.  Instead of implementing a new protocol for data exchange
  like AMQP, Dchan uses a simple file interface.  There's no need of
  client libraries for each language (every language knows how to read
  and write from files).

  Dchan is able to share those files in the network with the help of
  the 9P protocol.

  This project uses the concept of [[https://en.wikipedia.org/wiki/Literate_programming][Literate Programming]] of Donald
  Knuth.

#+BEGIN_QUOTE
Let us change our traditional attitude to the construction of
programs: Instead of imagining that our main task is to instruct a
computer what to do, let us concentrate rather on explaining to human
beings what we want a computer to do. \cite{Knuth:1984:LiterateProgramming}
#+END_QUOTE

  This book is the only source for design ideas, code, documentation and
  tests. From here, we build everything.

** Why dchan?

   Dchan was created as an alternative technology for data processing
   pipelines. The classic approach to data processing is to write a
   program that reads in data, transforms it in some desired way, and
   outputs new data. Such programs, when chained, forms an
   architecture called "data pipeline". An obvious example is UNIX
   pipes, that works very well when all programs share the same
   address space. For distributed architectures, a more elaborate
   solution must be used to perform message passing between different
   machines.

   After some time using RabbitMQ, and then studying other solutions,
   we discovered that majority of current technologies aren't good
   enough to solve our specific use cases.

   The problem is that, when the worker queue grow indefinitely,
   because of slow consumers throughput compared to publishers or
   because some consumers crashed, two things can happen:

   - Memory starvation (in case of no persistence)
   - Abruptly performance degradation

   In a high volume data processing, the difference of throughput
   between publishers and consumers can lead to lots of terabytes
   waiting to be processed if not properly handled.

   Dchan is an alternative to current brokered solutions but providing
   ad-hoc synchronization. Dchan helps micro-services to communicate
   transparently. It's much like a rendezvous point in your network,
   where different services met in order to exchange data directly (no
   need to store messages in-fly). Dchan uses the CSP (Communicating
   Sequential Processes) style of concurrency to create channels of
   data, enabling direct inter-process communication without requiring
   service discovery.

** Alternative technologies

   Below are some common used technologies for stream processing:

   - Broker (message oriented middlewares)
     - AMQP
       - RabbitMQ
       - Qpid
       - ActiveMQ
     - NATS
     - NSQ
     - Kafka
     - Redis
   - Brokerless (message passing libraries)
     - ZeroMQ
     - Nanomsg
     - Akka (actor concurrency model)
     - Erlang OTP

   The brokered solution could be splited into two classes:

   - With database capabilities (persistence, guarantees)
     - RabbitMQ
     - QPid
     - ActiveMQ
     - NSQ
     - Kafka
     - Redis
   - In-memory queues
     - NATS

   Using the brokered solutions listed above means that will exists a
   message queue between every program that consumes data. This queue
   is called /worker queue/ and can be in-memory or persistent
   depending on the solution.

   Some brokers with built-in database can be configured to work as
   in-memory queues too (RabbitMQ, Qpid, NSQ, Redis), but in this
   case, to avoid memory starvation, it should use some kind of
   watermark configuration, to activate persistence, or drop new
   messages, when the memory is over the threshold (See [[https://www.rabbitmq.com/memory.html][here]], [[http://nsq.io/deployment/production.html][here]],
   [[http://redis.io/topics/memory-optimization][here]] and [[https://qpid.apache.org/releases/qpid-0.32/cpp-broker/book/chapter-Managing-CPP-Broker.html][here]]).

   The entire application being down because of OOM in the broker
   isn't a good option for data processing, then NATS doesn't
   satisfies the basic requirement. NATS has a simple and good
   architecture but unusable for data processing because it turns the
   problem even worst, by automatically disconnecting slow consumers
   freeing their queues.

   The persistence option saves the broker of OOM, but the performance
   degradation turns the problem even worst for consumers, because now
   every delivery will hit the disk. For continuous data processing,
   this means that when the broker hit the watermark, it will never
   restore the performance of in-memory queues again and database will
   grow infinitely. In this case, a proper database is desirable
   instead of a message queue.

   Kafka is a distributed commit log service with strong guarantees
   that can be used as message broker. To avoid memory starvation
   kafka could be configured to flush data to disk at some time
   interval (log.flush.interval.ms) or flush when a specific amount of
   messages exists in memory (log.flush.interval.messages). The last
   config is much like watermark's configurations, but the
   difference is that it represents the amount of messages, not a size
   in memory. Kafka uses the configuration log.retention.hours to
   determine the amount of time to retain data on disk. The problem of
   slow consumers could be solved slowing down the producers with
   quota's settings (quota.publisher.default) being configured by some
   worker manager.

   Kafka apparently is the broker most prepared for high volume
   continuous data processing pipeline, but this have the cost of
   configurability and tuning.

   The feature that is lacking in all brokers is fixed-size queues
   with synchronization between publishers and consumers.

   Fixed size queues will avoid memory starvation and with synchronism
   the publishers will wait until the consumers are ready to get
   data. The performance of a data pipeline is best measured by the
   performance of the last services of the chaining, because they're
   generating the useful, ready-to-be-used, enriched data. If no
   bottleneck exists in the architecture, with synchronized services
   what we'll achieve is the same performance throughput across the
   entire pipeline (the performance of the slowest service).

   Using the brokerless alternatives cited before we can implement the
   required synchronism and avoid infinite queues, but this requires
   additional [[http://hintjens.com/blog:32][service discovery capabilities to every micro-service of the
   architecture]].

** Current situation

   Today we use RabbitMQ service for message passing inter-teams and
   at several places of architecture and apart from the broker
   problems explained in the last chapter, it proved to be hard
   to achieve high quality code.

   AMQP (Advanced Message Queue Protocol) is a complex bad designed
   specification protocol and because of that, client libraries are
   huge and sometimes buggy. On top of a huge library, the
   specification still imposes a lot of client code to achieve
   durability and reliability. That big amount of code (and tests)
   needs to be written in the correct manner and must be correctly
   tested. Testing is hard because the need for a central complete
   broker (not easy to mock with libraries) and some way to start/stop
   the broker for test re-connection and guarantees (durability). In
   simple words: hard to achieve good quality code.

   For more information about this kind of problems, read the article
   below from one of the AMQP creators:

   http://www.imatix.com/articles:whats-wrong-with-amqp/

   Other problem is that AMQP specification does not say any words
   about synchronism between publishers and consumers of queues, and
   the broker is designed to be a complete database to store the
   difference between throughput of clients. Sometimes this is a
   desired behavior, but sometimes it is not. If you have a low
   traffic messaging, it works, but using the message broker as a
   database for a large dataset processing requires more database
   capabilities in the broker than messaging (and AMQP is a messaging
   protocol).

** Desired goals

Dchan have the goals below:

- It must have a simple API;
- It must support text messages over the wire;
- It must support composability or inter-dchan communications;
- It must support unicast and multicast;
- It must be easy for testing;
- It must scale;
