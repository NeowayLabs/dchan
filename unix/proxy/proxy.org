* dchan-proxy

** Makefile

#+BEGIN_SRC makefile :tangle Makefile
all: clean tangle build test

tangle:
	org-tangle proxy.org

build: tangle
	go build -v

test: tangle build
	bash ../testing/testcover.sh
	chmod +x tests/acceptance/*.sh
	./tests/acceptance/vfs.sh

clean:
	rm -f tests/acceptance/*.sh
	rm -f *.go

#+END_SRC

** Proxy acceptance tests

   The most important acceptance tests for the proxy server are:

   - File system interface;
   - Network partitions with dchan server;

   Below are the VFS tests:

#+BEGIN_SRC sh :tangle tests/acceptance/vfs.sh :shebang #!/bin/bash
. ../testing/helpers.sh

BACKENDPID=""

function start9pserver {
    local cwd="$(pwd)"

    go get -v -u github.com/lionkov/go9p
    cd ${GOPATH}/src/github.com/lionkov/go9p/p/srv/examples/clonefs/
    go build -v
    ./clonefs -addr ":6666" -d &

    BACKENDPID=$!

    cd "${cwd}"
}

function changedir {
    local targetdir="$1"

    cd "${targetdir}"

    if [ "$?" != "0" ]; then
        echo "Failed to change dir"
        exit 1
    fi
}

# Running backend 9P file server

start9pserver

rm -f /tmp/dchan-proxy.sock
setup "/tmp/dchan-proxy" "unix!/tmp/dchan-proxy.sock" "$(pwd)/proxy -laddr unix:///tmp/dchan-proxy.sock -raddr 'localhost:6666'"

changedir "/tmp/dchan-proxy"

teardown "/tmp/dchan-proxy" "${DAEMONPID}"

kill ${BACKENDPID}
#+END_SRC

** Proxy implementation

   Dchan-proxy is a pure TCP proxy that knows nothing about 9P or
   dchan internal working. It only bypass local network packets (from
   unix socket or loopback tcp port) to remote destination server. The
   difference is that it never fail to clients. If the backend server
   crash or network link is down, it will not return any bytes to
   client until it successfully reconnects with backend server.

   The Go implementation of the proxy has heavy use of CSP channels to
   pipe data from one connection to the other, not interested in the
   content.

   The algorithm for this pipe is based on the following ideas:

   1. For the listener connection /lConn/, create a channel /lChan/ and
      a go-routine reading bytes from /lConn/ and writing into
      /lChan/.

   2. For the remote connection /rConn/, create a channel /rChan/ and
      a go-routine reading bytes from /rConn/ and writing into /rChan/.

   3. The piping process is an infinite loop with a non-deterministic
      choice (select) operation writing everything read from /lChan/
      into /rConn/ and everything read from /rChan/ into /lConn/.


    The first two steps are achieved by /chanFromConn/ function.

    ChanFromConn creates a channel /c/ and sends everything it reads from the socket
    connection /conn/ into /c/. It makes by creating a new go-routine
    for reading data on the connection and copying it to destination
    channel. Data is copied to avoid races with channel's consumer.

    If something bad occurs reading /conn/, then the channel /c/ is
    closed to notify the consumer that /conn/ is down, requiring a
    re-connect.

    It returns a receive-only (unidirectional) channel containing data
    read from connection.

#+NAME: src-proxy-core-fn-chanfromconn
#+BEGIN_SRC go
func chanFromConn(conn net.Conn) <-chan []byte {
	c := make(chan []byte)

	go func() {
		b := make([]byte, 1024)

		for {
			n, err := conn.Read(b)

			if n > 0 {
				res := make([]byte, n)
				copy(res, b[:n])
				c <- res
			}

			if err != nil {
				if err == io.EOF {
					// connection was closed
					close(c)
				} else {
					c <- nil
					close(c)
				}

				break
			}
		}
	}()

	return c
}
#+END_SRC

    ChanFromConn unit tests can be seen [[ChanFromConn testcases][here]].

    The third step is handled by /Pipe/ function.

    Pipe creates a full-duplex pipe between the two sockets /lConn/
    and /rConn/, and transfers data from one to the other (see
    [[Non-deterministic choice][Non-deterministic choice]]).

    Pipe uses the following convention to simplify the implementation:

    If a nil is received on /lChan/ and channel is closed, then this
    means that client disconnects.  If a nil is received on /lChan/
    but channel is still active, then some error happened in the
    connection.

    The same for the remote one: If a nil is received on /rChan/ and
    channel is closed, then this means that remote backend closed the
    connection. If a nil is received but the channel is still active,
    then some network error occurred.

#+NAME: src-proxy-core-fn-pipe
#+BEGIN_SRC go
func Pipe(lConn net.Conn, rConn net.Conn) (error, error)  {
	lChan := chanFromConn(lConn)
	rChan := chanFromConn(rConn)

	for {
		select {
		case b1, ok1 := <-lChan:
			if b1 == nil && ok1 == true {
				return errors.New("Local connection error"), nil
			} else if b1 == nil && ok1 == false {
				// connection succesfully closed
				return nil, nil
			} else {
				rConn.Write(b1)
			}
		case b2, ok2 := <-rChan:
			if b2 == nil && ok2 == true {
				return nil, errors.New("connection error")
			} else if b2 == nil && ok2 == false {
				return nil, nil
			} else {
				lConn.Write(b2)
			}
		}
	}

	panic("unreachable")
}
#+END_SRC

    Pipe unit tests can be seen [[Pipe testcases][here]].

*** Main

   Dchan-proxy receives only two command-line parameters:

   - laddr: URI to listen on
   - raddr: URI to TCP remote address

   By default it binds to unix socket on file /tmp/dchan-proxy.sock.

#+NAME: src-dchan-proxy-flags
#+BEGIN_SRC go
var (
	laddr *string = flag.String("laddr", "unix:///tmp/dchan-proxy.sock", "local address")
	raddr *string = flag.String("raddr", "", "remote address")
)
#+END_SRC

    The main function only parses the command line arguments and calls core.Start.

#+BEGIN_SRC go src-main.go :tangle main.go :noweb yes :main no
package main

import (
	"os"
	"fmt"
	"flag"
	"github.com/NeowayLabs/dchan/unix/proxy/core"
)

<<src-dchan-proxy-flags>>

func main() {
	var err error

	flag.Parse()

	if *raddr == "" {
		fmt.Printf("-raddr is required.\n")
		os.Exit(1)
	}

	err = core.Start(*laddr, *raddr)

	if err != nil {
		panic(err)
	}
}

#+END_SRC

*** Core package

    The core package is responsible for the magic. It starts the
    socket server and the go-routines to handle the requests.

    Start function first discover the type of socket for the local server to
    use the generic net.Listen function. Then it starts listening on
    configured local address for incoming connections.

    For every new connection, it calls handleProxy in a new
    go-routine. A new go-routine for each connection is required
    because 9P is a stateful protocol, this means the network
    connection will stay established until client disconnects (unmount
    the file system). We can limit the max number of clients in the
    future.

#+NAME: src-proxy-core-fn-start
#+BEGIN_SRC go
func Start(laddr, raddr string) error {
	var (
		nettype, addrval string
		err error
	)

	if laddr[0:7] == "unix://" {
		nettype = "unix"
		addrval = laddr[7:]
	} else if laddr[0:6] == "tcp://" {
		nettype = "tcp"
		addrval = laddr[6:]
	} else {
		nettype = "tcp"
		addrval = laddr
	}

	listener, err := net.Listen(nettype, addrval)

	if err != nil {
		panic(err)
	}

	for {
		conn, err := listener.Accept()

		if err != nil {
			panic(err)
		}

                go handleProxy(conn, raddr)
	}
}
#+END_SRC

    HandleProxy establish a new connection with the backend 9P server
    and starts piping data from remote socket to the local one using
    the Pipe function. When there's no more data to read or write to
    remote destination, handleProxy close both connections.

    The Pipe is based on the blog post below:

    https://www.stavros.io/posts/proxying-two-connections-go/

    It was not possible to use plain io.Copy because we have
    requirements about network failures.

#+NAME: src-proxy-core-fn-handleProxy
#+BEGIN_SRC go
func handleProxy(conn net.Conn, raddr string) {
	addr, err := net.ResolveTCPAddr("tcp", raddr)
	if err != nil {
		panic(err)
	}

	rConn, err := net.DialTCP("tcp", nil, addr)

	if err != nil {
		panic(err)
	}

	defer func() {
		rConn.Close()
		conn.Close()
        }()

	Pipe(conn, rConn)
}
#+END_SRC

#+HEADER: :imports '("net" "errors" "io")
#+HEADER: :package core
#+BEGIN_SRC go :tangle core/proxy.go :noweb yes :main no :comments yes :exports none
<<src-proxy-core-fn-chanfromconn>>
<<src-proxy-core-fn-pipe>>
<<src-proxy-core-fn-handleProxy>>
<<src-proxy-core-fn-start>>

#+END_SRC

** Core unit tests

   Proxy test coverage can be seen [[http://neowaylabs.github.io/dchan/proxy_cover.html][here]].

*** ChanFromConn testcases

   To Easy the testing, we'll create our own net.Conn implementation
   that only writes and read in a internal buffer.

   The /MyConn/ have a /mutex/ to synchronize reads and writes into
   /buffer/ and a /counter/ integer property to trigger a connection
   error when 5 (five) or more reads occurs. The newMockCon returns a
   new fresh connection.

#+NAME: src-proxy-core-tests-myconn-new
#+BEGIN_SRC go
type MyConn struct {
	buffer []byte
        *sync.Mutex

        counter int
        closed bool
}

func newMockConn() net.Conn {
	c := &MyConn{}
	c.buffer = make([]byte, 0, 1024)
	c.Mutex = &sync.Mutex{}
        return c
}
#+END_SRC

    Write and Read simply operate on internal byte array
    /buffer/. Both functions lock to avoid races.

#+NAME: src-proxy-core-tests-myconn-impl
#+BEGIN_SRC go
func (c *MyConn) Write(d []byte) (int, error) {
	c.Lock()
	defer c.Unlock()

	if c.closed {
		return 0, errors.New("Connection closed")
	}

	for _, b := range d {
		c.buffer = append(c.buffer, b)
	}

	return len(d), nil
}

func (c *MyConn) Read(d []byte) (int, error) {
	var i int

readAgain:
	c.Lock()

        if c.closed {
		return 0, io.EOF
	}

        if c.counter >= 5 {
		c.Unlock()
		return 0, errors.New("Connection error")
	}

        if len(c.buffer) == 0 {
		c.Unlock()
		time.Sleep(100 * time.Millisecond)
		goto readAgain
	}

	for i = 0; i < cap(d) && i < len(c.buffer); i++ {
		d[i] = c.buffer[i]
	}

	c.buffer = c.buffer[i:]
	c.counter += 1

	c.Unlock()

	return i, nil
}

func (c *MyConn) Close() error {
	c.Lock()
	defer c.Unlock()

	c.buffer = nil
        c.closed = true
	return nil
}

func (c *MyConn) LocalAddr() net.Addr { return nil }
func (c *MyConn) RemoteAddr() net.Addr { return nil }
func (c *MyConn) SetDeadline(t time.Time) error { return nil }
func (c *MyConn) SetReadDeadline(t time.Time) error { return nil }
func (c *MyConn) SetWriteDeadline(t time.Time) error { return nil }
#+END_SRC

   ChanFromConn must be tested for the following cases:

   1. Every data written into the connection must be written into the
     channel;
   2. If the Read from the connection fails, the channel must be
     closed;


   For the first case, we can test writing something into the
   connection and verifying if it was written into the channel.

#+NAME: src-proxy-core-tests-1
#+BEGIN_SRC go
func TestChanFromConn1(t *testing.T) {
	conn := newMockConn()
	chan1 := chanFromConn(conn)

	conn.Write([]byte("teste"))

        timeout := time.After(1 * time.Second)

	select {
	case d := <-chan1:
		if string(d) != "teste" {
			t.Errorf("Expected '%s' != from '%s'.",
				"teste", string(d))
		}
	case <-timeout:
		t.Error("No data available in 1 second")
	}

        conn.Write([]byte("i4k"))

        timeout = time.After(1 * time.Second)

        select {
	case d := <-chan1:
		if string(d) != "i4k" {
			t.Errorf("Expected '%s' != from '%s'.",
				"i4k", string(d))
		}
	case <-timeout:
		t.Error("No data available in 1 second")
	}
}
#+END_SRC

    For the second case, we'll write 5 times into the connection to
    trigger an error in the Read method of /MyConn/.

#+NAME: src-proxy-core-tests-2
#+BEGIN_SRC go
func TestChanfromconn2(t *testing.T) {
	conn := newMockConn()
	chan1 := chanFromConn(conn)

	conn.Write([]byte("you"))
	<-chan1
	conn.Write([]byte("have"))
	<-chan1
	conn.Write([]byte("been"))
	<-chan1
	conn.Write([]byte("hacked"))
	<-chan1
	conn.Write([]byte("!!!"))
        <-chan1

        // The next read will trigger a connection error
	v, ok := <-chan1

	if v == nil && ok == false {
		t.Errorf("The channel must be open... Returned %v :: %v", string(v), ok)
	}
}
#+END_SRC

#+NAME: src-proxy-core-tests-chanfromconn
#+HEADER: :imports '("net" "io" "testing" "time" "sync" "errors")
#+BEGIN_SRC go :noweb yes :tangle core/chanfromconn_test.go :comments yes :package core :main no :exports none

<<src-proxy-core-tests-myconn-new>>
<<src-proxy-core-tests-myconn-impl>>

<<src-proxy-core-tests-1>>
<<src-proxy-core-tests-2>>
#+END_SRC

*** Pipe testcases

    The Pipe function have the following test cases:

    1. Every byte written on one channel must be written on the other;
    2. If reading some of the channels receive nil, but the channel is
       closed, then Pipe must return successfully;
    3. If reading some of the channels receive nil but channel is
       active, then must return an error;

#+NAME: src-proxy-core-tests-pipe-1
#+BEGIN_SRC go
func TestPipe1(t *testing.T) {
	lConn := newMockConn()
	rConn := newMockConn()

        go func() {
		err1, err2 := Pipe(lConn, rConn)

		if err1 != nil {
			t.Errorf("Conn1 failed: %s", err1.Error())
		}

                if err2 != nil {
			t.Errorf("Conn2 failed: %s", err2.Error())
		}
	}()

	lConn.Write([]byte("teste"))

	data := make([]byte, 5)

	n, err := rConn.Read(data)

	if err != nil {
		t.Error(err)
		return
	}

	if n != 5 {
		t.Errorf("Expected 5 bytes, received %d", n)
		return
	}

	if string(data) != "teste" {
		t.Errorf("Expected '%s' but received '%s'",
			"teste", string(data))
	}
}
#+END_SRC

    Closing the local connection must make Pipe return successfully
    (second case).

#+NAME: src-proxy-core-tests-pipe2lconn
#+BEGIN_SRC go
func TestPipe2LConn(t *testing.T) {
	lConn := newMockConn()
	rConn := newMockConn()

	done := make(chan bool)

	go func() {
		err1, err2 := Pipe(lConn, rConn)

		if err1 != nil {
			t.Error("conn1 failed: %s", err1.Error())
		}

		if err2 != nil {
			t.Error("conn2 failed: %s", err2.Error())
		}

		done <- true
	}()

	lConn.Close()

	<-done
}
#+END_SRC

    Closing the remote connection must make Pipe return successfully
    (second case).

#+NAME: src-proxy-core-tests-pipe2rconn
#+BEGIN_SRC go
func TestPipe2RConn(t *testing.T) {
	lConn := newMockConn()
	rConn := newMockConn()

	done := make(chan bool)

	go func() {
		err1, err2 := Pipe(lConn, rConn)

		if err1 != nil {
			t.Error("conn1 failed: %s", err1.Error())
		}

		if err2 != nil {
			t.Error("conn2 failed: %s", err2.Error())
		}

		done <- true
	}()

	rConn.Close()

	<-done
}
#+END_SRC

    If some network error happens on the local socket, then Pipe
    should fail and return the error on /lErr/ and /rErr/ should be /nil/.

#+NAME: src-proxy-core-tests-pipe3lconn
#+BEGIN_SRC go
func TestPipe3LConn(t *testing.T) {
	lConn := newMockConn()
	rConn := newMockConn()

	done := make(chan bool)

	go func() {
		lErr, rErr := Pipe(lConn, rConn)

		if lErr == nil {
			t.Error("conn1 should fail...")
		}

		if rErr != nil {
			t.Error("conn2 failed: %s", rErr.Error())
		}

		done <- true
	}()

        lConn.Write([]byte("this"))
        lConn.Write([]byte("will"))
        lConn.Write([]byte("trigger"))
        lConn.Write([]byte("an"))
        lConn.Write([]byte("error"))

	<-done
}
#+END_SRC

    If some network error happens on the local socket, then Pipe
    should fail and return the error on /lErr/ and /rErr/ should be /nil/.

#+NAME: src-proxy-core-tests-pipe3rconn
#+BEGIN_SRC go
func TestPipe3RConn(t *testing.T) {
	lConn := newMockConn()
	rConn := newMockConn()

	done := make(chan bool)

	go func() {
		lErr, rErr := Pipe(lConn, rConn)

		if lErr != nil {
			t.Error("lErr failed: %s", lErr.Error())
		}

		if rErr == nil {
			t.Error("conn1 should fail...")
		}

		done <- true
	}()

        rConn.Write([]byte("this"))
        rConn.Write([]byte("will"))
        rConn.Write([]byte("trigger"))
        rConn.Write([]byte("an"))
        rConn.Write([]byte("error"))

	<-done
}
#+END_SRC

#+NAME: src-proxy-core-tests-pipe
#+HEADER: :main no :package core
#+HEADER: :imports '("testing")
#+BEGIN_SRC go :noweb yes :tangle core/pipe_test.go
<<src-proxy-core-tests-pipe-1>>

<<src-proxy-core-tests-pipe2lconn>>
<<src-proxy-core-tests-pipe2rconn>>
<<src-proxy-core-tests-pipe3lconn>>
<<src-proxy-core-tests-pipe3rconn>>
#+END_SRC
